# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01_parser.ipynb (unless otherwise specified).

__all__ = ['path_type', 'bin_val', 'bytes_encoded', 'datetime_object', 'parse_bin', 'classify_blocks',
           'decode_compressed_block', 'run_length_decode5', 'extract_level', 'meta2df', 'extract_metadata',
           'export_meta', 'export_level']

# Cell
from multiprocessing import set_start_method, Pool
try:
    set_start_method("spawn")
except RuntimeError:
    pass
import os
from pathlib import Path
from typing import *
from datetime import datetime
from collections import defaultdict
from functools import partial
from fastcore.basics import uniqueify
from fastcore.utils import parallel
from fastcore.foundation import L
from rich.progress import track
from .blocks import *
from .utils import *
from .constants import *
from .cyparser import cy_extract_compressed
import pandas as pd
import numpy as np
path_type = Union[str, Any]
bin_val = Union[int, bytes]
bytes_encoded = Union[int, bytes]
datetime_object = datetime

# Cell
def parse_bin(bin_file,
              bytes_header: int = BYTES_HEADER,
              marker: bytes = ENDMARKER,
              slice_=None,
              progress=None):
    with open(bin_file, mode='rb') as bfile:
        # O primeiro bloco do arquivo é o cabeçalho e tem 36 bytes de tamanho.
        header = bfile.read(bytes_header)
        body = bfile.read()
    if slice_ is not None:
        assert slice_.start >= bytes_header, f"The start of your slice has to be >= {bytes_header}, you passed {slice_.start} "
        body = body[slice_]
    return {'file_version': bin2int(header[:4]), 'string': bin2str(header[4:]), 'blocks': classify_blocks(body.split(marker),progress)}

# Cell
def classify_blocks(blocks: list, progress=None)->Mapping[int,L]:
    """Receives an iterable L with binary blocks and returns a defaultdict with a tuple (block types, thread_id) as keys and a list of the Class Blocks as values
        :param file: A string or pathlib.Path like path to a `.bin`file generated by CFRS - Logger
        :return: A Dictionary with block types as keys and a list of the Class Blocks available as values
    """
    map_block = defaultdict(L)
    index = BYTES_HEADER
    if progress:
        tracker = progress.track(blocks, total=len(blocks), description=progress.description)
    else:
        tracker = blocks
    for block in tracker:
        bloco = create_base_block(block)
        btype, btid = bloco.type, bloco.thread_id
        stop = index + DATA_BLOCK_HEADER + len(bloco.data) + CHECKSUM
        if btype not in MAIN_BLOCKS:
            index = stop + LEN_MARKER
            continue
        map_block[(btype, btid)].append(((index,stop), block_constructor(btype, bloco)))
        index = stop + LEN_MARKER
    return map_block

# Cell
def decode_compressed_block(block):
    MIN = block.offset - 127.5
    RUN = 255
    ESC = 254
    src = block.data[block.start:block.stop]
    nsrc = len(src)
    thresh = block.thresh
    dest = np.full(block.norig, MIN, dtype=np.float16)
    i = 0
    j = 0
    while i < nsrc:
        ib = src[i]
        i+=1
        if ib == RUN:
            nrun = src[i]
            i+=1
            dest[j:j+nrun] = MIN + thresh/2.
            j+=nrun
        elif ib == ESC:
            # next value is literal
            dest[j] = MIN + src[i]/2.
            i+=1 ; j+=1
        else:
            # value
            dest[j] = MIN + ib/2.
            j+=1
    return dest

# Cell
def run_length_decode5(dest, src, nsrc, thresh, MIN):
    i = 0
    j = 0
    while i < nsrc:
        ib = src[i]
        i+=1
        if ib == RUN:
            nrun = src[i]
            i+=1
            dest[j:j+nrun] = thresh #dest is now a numpy array
            j+=nrun
        elif ib == ESC:
            # next value is literal
            dest[j] = src[i]/2. + MIN
            i+=1 ; j+=1
        else:
            # value
            dest[j] = ib/2. + MIN
            j+=1

# Cell
def _extract_uncompressed(blocks: Iterable, rows: int, cols: int, MIN: float, dtype=np.float16):
    levels = np.full((rows, cols), MIN, dtype=dtype)
    for b, block in enumerate(blocks):
        levels[b] = block.block_data
    return levels

def extract_level(spectrum_blocks: Iterable, dtype=np.float32)->pd.DataFrame:
    """Receives a mapping `spectrum_blocks` and returns the Matrix with the Levels as values, Frequencies as columns and Block Number as index.
       :param pivoted: If False, optionally returns an unpivoted version of the Matrix
    """
    assert len(spectrum_blocks), f"The spectrum block list is empty"
    spectrum_blocks = spectrum_blocks.itemgot(1)
    block = spectrum_blocks[0]
    assert block.type in (63, 64, 67, 68), f"The input blocks are not spectral blocks"
    rows = len(spectrum_blocks)
    MIN = block.offset - 127.5
    if block.type in (63,67):
        cols = block.ndata
        frequencies = getattr(block, 'frequencies')
        return pd.DataFrame(_extract_uncompressed(spectrum_blocks, rows, cols, MIN, dtype), columns=frequencies)
    else:
        cols = block.norig
        thresh = block.thresh - 1
        block_data = [b.block_data for b in spectrum_blocks]
        frequencies = np.linspace(block.start_mega, block.stop_mega, num=cols)
        levels = cy_extract_compressed(block_data, rows, cols, thresh, MIN)
        if dtype != np.float32:
            levels = levels.astype(dtype)
        return pd.DataFrame(levels, columns=frequencies)

# Cell
def meta2df(meta_list: Iterable, optimize: bool = True)->pd.DataFrame:
    """Receives and Iterable `metalist` with metadata and converts it to a DataFrame"""
    df = pd.DataFrame(meta_list)
    dt_features = ['wallclock_datetime'] if 'wallclock_datetime' in df.columns else []
    if optimize:
        df = df_optimize(df, dt_features)
    if dt_features:
        df = df.set_index('wallclock_datetime')
    return df

# Cell
def extract_metadata(blocos: L, attrs: list=None)->pd.DataFrame:
    """Receives a list of blocks, extracts the metadata from them and return a DataFrame
    """
    df = meta2df(blocos.map(lambda item: {**{'start_byte': item[0][0],
                                             'stop_byte': item[0][1]},
                                          **getattrs(item[1], attrs)}))
    return df

# Cell
from loguru import logger
def export_meta(stem: Union[str, Path],
                parsed_bin: dict,
                saida: Union[str, Path],
                ext: str = '.csv')-> None:

    file_version, string, blocks = parsed_bin.values()
    func = partial(_export_meta, stem=stem, saida=saida, ext=ext)
    parallel(func, list(blocks.items()), n_workers=os.cpu_count(), pause=0.5)

def _export_meta(parsed_blocks: tuple,
                stem: Union[str, Path],
                saida: Union[str, Path],
                ext: str = '.csv')->Union[None, pd.DataFrame]:

    (tipo, tid), blocos = parsed_blocks
    meta = extract_metadata(blocos)
    dt_features = isinstance(meta.index, pd.DatetimeIndex)

#     if dt_features:
#         start = meta.index.min().strftime("%Y%m%dT%H%M%S")
#         end = meta.index.max().strftime("%Y%m%dT%H%M%S")
#         name = f'{stem}-B_{tipo}_TId_{tid}-{start}_{end}'
#     else:
    name = f'{stem}-B_{tipo}_TId_{tid}'

    if ext == '.csv':
        meta.to_csv(Path(saida) / f'{name}{ext}', index=bool(dt_features))
    elif ext == '.xlsx':
        meta.to_excel(Path(saida) / f'{name}{ext}', index=bool(dt_features))
    elif ext == '.fth':
        meta.reset_index().to_feather(Path(saida) / f'{name}{ext}')
    else:
        raise ValueError(f"Extension {ext} not implemented")

# Cell
def export_level(stem: Union[str, Path],
            parsed_bin: dict,
            saida: Union[str, Path],
            ext: str = '.fth',
            index: pd.DatetimeIndex = None)-> None:

    file_version, string, blocks = parsed_bin.values()
    func = partial(_export_level, stem=stem, saida=saida, ext=ext, index=index)
    parallel(func, list(blocks.items()), n_workers=os.cpu_count(), pause=0.5)

@logger.catch
def _export_level(parsed_blocks: tuple,
                 stem: Union[str, Path],
                 saida: Union[str, Path],
                 ext: str = '.fth',
                 index: pd.DatetimeIndex = None)->Union[None, pd.DataFrame]:

    (tipo, tid), blocos = parsed_blocks
    level = extract_level(blocos)
    if index is not None:
        level.index = index
    dt_features = isinstance(level.index, pd.DatetimeIndex)

    name = f'{stem}-B_{tipo}_TId_{tid}'
    if ext == '.xlsx':
        level.to_excel(Path(saida) / f'{name}{ext}', index=bool(dt_features))
    elif ext == '.fth':
        level.reset_index().to_feather(f'{saida}/{name}{ext}')
    else:
        raise ValueError(f"Extension {ext} not implemented")