{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from collections import defaultdict\n",
    "from fastcore.basics import uniqueify\n",
    "from fastcore.utils import parallel\n",
    "from fastcore.foundation import L\n",
    "from rfpy.blocks import create_base_block, block_constructor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "from datetime import datetime\n",
    "\n",
    "path_type = Union[str, Any]\n",
    "bin_val = Union[int, bytes]\n",
    "bytes_encoded = Union[int, bytes]\n",
    "datetime_object = datetime\n",
    "\n",
    "META = {'Block_Number': 'uint16',\n",
    "     'Latitude': 'float32',\n",
    "     'Longitude': 'float32',\n",
    "     'Altitude': 'float16',\n",
    "     'Initial_Time': 'datetime64[ns]',\n",
    "     'Sample_Duration': 'uint16',\n",
    "     'Start_Frequency': 'uint32',\n",
    "     'Stop_Frequency': 'uint32',\n",
    "     'Vector_Length': 'uint16',\n",
    "     'Trace_Type': 'category',\n",
    "     'Antenna_Type': 'category',\n",
    "     'Equipement_ID': 'category'}\n",
    "\n",
    "LEVELS = {'Block_Number': 'category', 'Frequency(MHz)': 'category', \"Nivel(dBm)\" : 'float16'}\n",
    "\n",
    "COLS_B = ['Block_Number', 'Frequency(MHz)', 'Nivel(dBm)']\n",
    "\n",
    "BLOCO_40 = ['latitude', 'longitude', 'altitude']\n",
    "\n",
    "BLOCO_63 = ['datetime_stamp',\n",
    "           'spent_time_microsecs', 'start_mega', 'stop_mega', 'data_points', \n",
    "           'processing', 'id_antenna']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonte: # https://github.com/fastai/fastai/blob/master/fastai/data/transforms.py#L26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_files(p, fs, extensions=None):\n",
    "    p = Path(p)\n",
    "    res = [p/f for f in fs if not f.startswith('.')\n",
    "           and ((not extensions) or f'.{f.split(\".\")[-1].lower()}' in extensions)]\n",
    "    return res\n",
    "\n",
    "def get_files(path, extensions=None, recurse=True, folders=None, followlinks=True):\n",
    "    \"Get all the files in `path` with optional `extensions`, optionally with `recurse`, only in `folders`, if specified.\"\n",
    "    path = Path(path)\n",
    "    folders=L(folders)\n",
    "    if extensions is not None:\n",
    "        extensions = set(uniqueify(extensions))\n",
    "        extensions = {e.lower() for e in extensions}\n",
    "    if recurse:\n",
    "        res = []\n",
    "        for i,(p,d,f) in enumerate(os.walk(path, followlinks=followlinks)): # returns (dirpath, dirnames, filenames)\n",
    "            if len(folders) !=0 and i==0: d[:] = [o for o in d if o in folders]\n",
    "            else:                         d[:] = [o for o in d if not o.startswith('.')]\n",
    "            if len(folders) !=0 and i==0 and '.' not in folders: continue\n",
    "            res += _get_files(p, f, extensions)\n",
    "    else:\n",
    "        f = [o.name for o in os.scandir(path) if o.is_file()]\n",
    "        res = _get_files(path, f, extensions)\n",
    "    return L(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def binary_file_generator(bin_file: path_type, marker: bytes = b'\\x00UUUU', block_size: int = 4096) -> Iterator[bytes]:\n",
    "    \"\"\"\n",
    "    str, bytes, int > bytes\n",
    "    :param bin_file: arquivo binario que contém os dados\n",
    "    :param marker: separador de blocos\n",
    "    :param block_size: tamanho em bytes que é \"lido\" por vez no arquivo, evitando problemas de memória\n",
    "    :return: bloco em formato binario\n",
    "    Gerador que fornece a partir de de um arquivo binário, um bloco binário por vez.\n",
    "\n",
    "    \"\"\"\n",
    "    with open(bin_file, mode='rb') as bfile:\n",
    "        # O primeiro bloco do arquivo é o cabeçalho e tem 36 bytes de tamanho.\n",
    "        yield bfile.read(36)\n",
    "        # As demais partes podem prosseguir normalmente\n",
    "        current = b''\n",
    "        while True:\n",
    "            block = bfile.read(block_size)\n",
    "            if not block:  # end-of-file\n",
    "                # yield current\n",
    "                return None\n",
    "            current += block\n",
    "            while True:\n",
    "                markerpos = current.find(marker)\n",
    "                if markerpos < 0:\n",
    "                    break\n",
    "                yield current[:markerpos]\n",
    "                current = current[markerpos + len(marker):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função a seguir mapeia o arquivo `.bin` nos devidos blocos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def file2block(file: Union[str, Path])->Mapping[int,L]:\n",
    "    \"\"\"Receives a path to a bin file and returns a defaultdict with unique block types as keys and a list of the Class Blocks as values\n",
    "        :param file: A string or pathlib.Path like path to a `.bin`file generated by CFRS - Logger\n",
    "        :return: A Dictionary with block types as keys and a list of the Class Blocks available as values\n",
    "    \"\"\"\n",
    "    map_block = defaultdict(L)\n",
    "    for bloco in L(binary_file_generator(file)).map(create_base_block):\n",
    "        btype = bloco.type\n",
    "        map_block[btype].append(block_constructor(btype, bloco))\n",
    "    return map_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below borrowed from https://medium.com/bigdatarepublic/advanced-pandas-optimize-speed-and-memory-a654b53be6c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def optimize_floats(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    floats = df.select_dtypes(include=['float64']).columns.tolist()\n",
    "    df[floats] = df[floats].apply(pd.to_numeric, downcast='float')\n",
    "    return df\n",
    "\n",
    "\n",
    "def optimize_ints(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ints = df.select_dtypes(include=['int64']).columns.tolist()\n",
    "    df[ints] = df[ints].apply(pd.to_numeric, downcast='integer')\n",
    "    return df\n",
    "\n",
    "\n",
    "def optimize_objects(df: pd.DataFrame, datetime_features: List[str]) -> pd.DataFrame:\n",
    "    for col in df.select_dtypes(include=['object']):\n",
    "        if col not in datetime_features:\n",
    "            num_unique_values = len(df[col].unique())\n",
    "            num_total_values = len(df[col])\n",
    "            if float(num_unique_values) / num_total_values < 0.5:\n",
    "                df[col] = df[col].astype('category')\n",
    "        else:\n",
    "            df[col] = pd.to_datetime(df[col])\n",
    "    return df\n",
    "\n",
    "def optimize(df: pd.DataFrame, datetime_features: List[str] = []):\n",
    "    return optimize_floats(optimize_ints(optimize_objects(df, datetime_features)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _extract_level(bloco: Any):\n",
    "    return np.expand_dims(bloco.block_data, 0) # reshape((-1, 1))\n",
    "     \n",
    "def export_bin_level(map_block: Mapping[int,L], pivoted: bool = True, freq_dtype: str = 'float32')->pd.DataFrame:\n",
    "    \"\"\"Receives a mapping `map_block` and returns the Matrix with the Levels as values, Frequencies as columns and Block Number as index.\n",
    "       :param pivoted: If False, optionally returns an unpivoted version of the Matrix\n",
    "    \"\"\"\n",
    "    spectrum_blocks = map_block[63]\n",
    "    assert len(spectrum_blocks), f\"The spectrum block list is empty\"\n",
    "    levels = np.concatenate(parallel(_extract_level, spectrum_blocks, n_workers=8, progress=False))\n",
    "    frequencies = getattr(spectrum_blocks[0], 'frequencies')\n",
    "    pivot = pd.DataFrame(levels, columns=frequencies)\n",
    "    if not pivoted:\n",
    "#         unpivot = pivot.melt(var_name=\"Frequency(MHz)\", value_name=\"Nivel(dBm)\")\n",
    "#         unpivot['Block_Number'] = levels.shape[0] * list(range(levels.shape[1]))\n",
    "#         unpivot.sort_values(['Block_Number', 'Frequency(MHz)'], inplace=True)\n",
    "#         unpivot.reset_index(drop=True, inplace=True)\n",
    "#         unpivot =  unpivot.astype({'Block_Number': 'category', 'Frequency(MHz)': 'float32', \"Nivel(dBm)\" : 'float16'})\n",
    "        # Doing directly as numpy is faster than pandas\n",
    "        unpivot = np.array([np.repeat(np.arange(levels.shape[1]), levels.shape[0]), \n",
    "                            np.tile(frequencies, levels.shape[0]),\n",
    "                            levels.flatten()]).T\n",
    "        return pd.DataFrame(unpivot, columns=COLS_B).astype(LEVELS)\n",
    "    return pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def export_bin_meta(map_block: Mapping[int,L])->pd.DataFrame:\n",
    "    \"\"\"Receives a Mapping with the different `. bin` Blocks and extracts the metadata listed in `META` in a dataframe format\n",
    "    \"\"\"\n",
    "    if not len(map_block[63]):\n",
    "        warnings.warn(\"Check if your Binary File is of type 63. GPS Block 40 length != Spectrum Block 63\")\n",
    "    n_data = len(map_block[63])\n",
    "    output = {}\n",
    "    output['Block_Number'] = L(range(n_data))     \n",
    "    for attr in BLOCO_40:\n",
    "        output[attr] = map_block[40].attrgot(attr)\n",
    "    for attr in BLOCO_63:\n",
    "        output[attr] = map_block[63].attrgot(attr)\n",
    "    output['Equipement_ID'] = L(getattr(map_block[21][0], 'hostname')) * n_data    \n",
    "    df = pd.DataFrame(output)    \n",
    "    df.columns = META.keys()\n",
    "#     for col, dtype in zip(df.columns, META_DTYPE):\n",
    "#         df[col] = df[col].astype(dtype)\n",
    "    return df.astype(META)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_main.ipynb.\n",
      "Converted 01_parser.ipynb.\n",
      "Converted 02_utils.ipynb.\n",
      "Converted 03_blocks.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rfpy]",
   "language": "python",
   "name": "conda-env-rfpy-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
