{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Console Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2            #Reload the code automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from datetime import datetime\n",
    "from typing import *\n",
    "import os\n",
    "import logging\n",
    "from fastcore.xtras import Path\n",
    "from fastcore.script import call_parse, Param, store_true\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rich.progress import Progress\n",
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "from rich.logging import RichHandler\n",
    "from rfpy.utils import *\n",
    "from rfpy.constants import SPECTRAL_BLOCKS\n",
    "from rfpy.parser import *\n",
    "CACHE_FOLDER = Path(r\"C:\\Users\\rsilva\\Downloads\\saida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "logging.basicConfig(\n",
    "    level=\"NOTSET\",\n",
    "    format=\"%(message)s\",\n",
    "    datefmt=\"[%X]\",\n",
    "    handlers=[RichHandler(rich_tracebacks=True)]\n",
    ")\n",
    "\n",
    "log = logging.getLogger(\"rich\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "custom_theme = Theme({\"info\": \"dim cyan\", \"warning\": \"magenta\", \"danger\": \"bold red\"})\n",
    "console = Console(theme=custom_theme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_meta(filename):\n",
    "    ext = filename.suffix\n",
    "    if ext == \".csv\":\n",
    "        df = pd.read_csv(filename)\n",
    "    elif ext == \".xlsx\":\n",
    "        df = pd.read_excel(filename, engine=\"openpyxl\")\n",
    "    elif ext == \".fth\":\n",
    "        df = pd.read_feather(filename)\n",
    "        if \"wallclock_datetime\" in df.columns:\n",
    "            df.set_index(\"wallclock_datetime\", inplace=True)\n",
    "    else:\n",
    "        raise ValueError(f\"Extension {ext} not implemented\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def filter_spectrum(df, start, stop, freq_start, freq_stop):\n",
    "    df = df.copy()\n",
    "    try:\n",
    "        start = pd.to_datetime(start)\n",
    "        stop = pd.to_datetime(stop)\n",
    "    except pd.errors.ParserError:\n",
    "        log.error(f\"[bold red blink] Datas inválidas! Verifique as strings de data {start} e {stop}\")\n",
    "        \n",
    "    try:\n",
    "        df.set_index('index', inplace=True)\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "    except pd.errors.KeyError:\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            log.warning(\n",
    "                f\"Não foi passado uma coluna ou índice com datetime a ser filtrado, todas as linhas serão processadas\",\n",
    "                exc_info=True\n",
    "            )\n",
    "            start = 0\n",
    "            stop = df.shape[0]\n",
    "\n",
    "    cols = df.columns.values.astype('float')\n",
    "    rows = df.index.values\n",
    "\n",
    "    filtered_cols = df.columns[(float(freq_start) <= cols) & (cols <= float(freq_stop))]\n",
    "    filtered_rows = df.index[(start <= rows) & (rows <= stop)]\n",
    "    if len(filtered_cols) == 0 or len(filtered_rows) == 0:\n",
    "        return None\n",
    "    count = filtered_rows.shape[0]\n",
    "    array = df.loc[filtered_rows, filtered_cols].values\n",
    "    freq = filtered_cols.values.astype('float32')\n",
    "    min_ = array.min(axis=0)\n",
    "    max_ = array.max(axis=0)\n",
    "    mean = array.mean(axis=0)\n",
    "    return pd.DataFrame({'Frequency': freq, 'Min': min_, 'Max': max_, 'Mean': mean, 'Count': count})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def appended_mean(row):\n",
    "    return (row['Count'] * row['Mean']).sum() / row['Count'].sum()\n",
    "\n",
    "@call_parse\n",
    "def extract_bin_stats(filename: Param(\"Caminho para o arquivo .bin\", str), \n",
    "                      start: Param(\"Timestamp do Início\", str), \n",
    "                      stop: Param(\"Timestamp do Fim\", str), \n",
    "                      freq_start: Param(\"Frequência Inicial (MHz)\", str),\n",
    "                      freq_stop: Param(\"Frequência Final (MHz)\", str)):\n",
    "\n",
    "    filename = Path(filename)\n",
    "    while True:\n",
    "        cached_files = get_files(CACHE_FOLDER / 'levels')\n",
    "        #TODO filter based on metadata\n",
    "        cached_levels = cached_files.filter(lambda name: filename.stem in str(name))\n",
    "        if not cached_levels:\n",
    "            process_bin(filename, CACHE_FOLDER, levels=True)\n",
    "        else:\n",
    "            break\n",
    "    dfs = cached_levels.map(pd.read_feather)\n",
    "    spectra = dfs.map(filter_spectrum, start=start, stop=stop, freq_start=freq_start, freq_stop=freq_stop)\n",
    "    spectra = [s for s in spectra if s is not None]\n",
    "    out = pd.DataFrame(columns=['Frequency', 'Min', 'Max', 'Mean'])\n",
    "    if not spectra:\n",
    "        log.warning(\n",
    "                f\"Os parâmetros repassados não correspondem a nenhum dado espectral do arquivo\",\n",
    "                exc_info=True\n",
    "            )\n",
    "        return out\n",
    "    spectra = pd.concat(spectra)\n",
    "    gb  = spectra.groupby('Frequency')\n",
    "    out['Frequency'] = spectra.Frequency.unique()\n",
    "    out['Min'] = gb.min()['Min'].values\n",
    "    out['Max'] = gb.max()['Max'].values\n",
    "    out['Mean'] = gb.apply(appended_mean).values\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@call_parse\n",
    "def process_bin(\n",
    "    entrada: Param(\"Arquivo .bin ou Diretório contendo arquivos .bin\", str),\n",
    "    saida: Param(\"Diretório para salvar os arquivos de saída\", str),\n",
    "    recursivo: Param(\"Buscar arquivos de maneira recursiva?\", store_true) = False,\n",
    "    pastas: Param(\"Limitar a busca às pastas\", Iterable[str]) = None,\n",
    "    levels: Param(\"Extrair e Salvar os níveis de Espectro?\", store_true) = False,\n",
    "    meta_ext: Param(\"Extensão do arquivo de metadados\", str) = \".fth\",\n",
    "    levels_ext: Param(\"Extensão do arquivo de níveis\", str) = \".fth\",\n",
    "    substituir: Param(\n",
    "        \"Reprocessar e substituir arquivos existentes?\", store_true) = False,\n",
    "    dtype: Param(\"Tipo de Dado ao salvar o arquivo de nível\", str) = 'float16'\n",
    "):\n",
    "    entrada = Path(entrada)\n",
    "    if entrada.is_file():\n",
    "        lista_bins = [entrada]\n",
    "    else:\n",
    "        lista_bins = get_files(\n",
    "            entrada, extensions=[\".bin\"], recurse=recursivo, folders=pastas\n",
    "        )\n",
    "    parsed_bins = {}\n",
    "    meta_path = Path(f\"{saida}/meta\")\n",
    "    levels_path = Path(f\"{saida}/levels\")\n",
    "    meta_path.mkdir(exist_ok=True, parents=True)\n",
    "    levels_path.mkdir(exist_ok=True, parents=True)\n",
    "    log_meta = Path(f\"{saida}/log_meta.txt\")\n",
    "    log_levels = Path(f\"{saida}/log_levels.txt\")\n",
    "    if substituir:\n",
    "        done_meta = set()\n",
    "        done_levels = set()\n",
    "    else:\n",
    "\n",
    "        done_meta = (\n",
    "            set(log_meta.read_text().split(\"\\n\")) if log_meta.exists() else set()\n",
    "        )\n",
    "        done_levels = (\n",
    "            set(log_levels.read_text().split(\"\\n\")) if log_levels.exists() else set()\n",
    "        )\n",
    "\n",
    "    console.rule(\"Lista de Arquivos a serem processados\", style=\"bold red\")\n",
    "    console.print(\n",
    "        [f.name for f in lista_bins],\n",
    "        style=\"bold white\",\n",
    "        overflow=\"fold\",\n",
    "        justify=\"left\",\n",
    "    )\n",
    "    if not lista_bins:\n",
    "        console.print(\":sleeping: Nenhum arquivo .bin a processar :zzz:\")\n",
    "        return\n",
    "\n",
    "    if not levels:\n",
    "        lista_bins = [f for f in lista_bins if f.name not in done_meta]\n",
    "    else:\n",
    "        lista_bins = [f for f in lista_bins if f.name not in done_levels]\n",
    "\n",
    "    if not lista_bins:\n",
    "        console.print(\":sleeping: Nenhum arquivo novo a processar :zzz:\")\n",
    "        console.print(\n",
    "            \":point_up: use --substituir no terminal ou substituir=True na chamada caso queira reprocessar os bins e sobrepôr os arquivos existentes :wink:\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    try:\n",
    "\n",
    "        with Progress(transient=True, auto_refresh=False) as progress:\n",
    "            bins = progress.track(\n",
    "                lista_bins,\n",
    "                total=len(lista_bins),\n",
    "                description=\"[green]Processando Blocos Binários\",\n",
    "            )\n",
    "\n",
    "            for file in bins:\n",
    "                progress.console.print(f\"[cyan]Processando Blocos de: [red]{file.name}\")\n",
    "                parsed_bins[file.name] = parse_bin(file)\n",
    "                progress.refresh()\n",
    "\n",
    "            lista_meta = [(k,v) for k,v in parsed_bins.items() if k not in done_meta]\n",
    "\n",
    "            if lista_meta:\n",
    "                blocks = progress.track(\n",
    "                    lista_meta, total=len(lista_meta), description=\"[cyan]Exportando Metadados\"\n",
    "                )\n",
    "                for file, block in blocks:\n",
    "                    progress.console.print(\n",
    "                        f\"[cyan]Extraindo Metadados de: [red]{file}\"\n",
    "                    )\n",
    "                    export_meta(\n",
    "                        file, block, meta_path, ext=meta_ext\n",
    "                    )\n",
    "                    done_meta.add(file)\n",
    "                    progress.refresh()\n",
    "            if levels:\n",
    "                lista_levels = lista_meta = [(k,v) for k,v in parsed_bins.items() if k not in done_levels]\n",
    "                if lista_levels:\n",
    "                    bins = progress.track(\n",
    "                        lista_levels,\n",
    "                        total=len(lista_levels),\n",
    "                        description=\"[grey]Exportando Dados de Espectro\",\n",
    "                    )\n",
    "                    for file, block_obj in bins:\n",
    "                        progress.console.print(\n",
    "                            f\"[grey]Extraindo Espectro de: [red]{file}\"\n",
    "                        )\n",
    "                        meta_index = []\n",
    "                        blocks = block_obj[\"blocks\"]\n",
    "                        for (tipo, tid) in blocks.keys():\n",
    "                            if tipo not in SPECTRAL_BLOCKS:\n",
    "                                continue\n",
    "                            meta_file = Path(\n",
    "                                f\"{meta_path}/{file}-B_{tipo}_TId_{tid}{meta_ext}\"\n",
    "                            )\n",
    "                            if not meta_file.exists():\n",
    "                                export_meta(\n",
    "                                    file,\n",
    "                                    block_obj,\n",
    "                                    meta_path,\n",
    "                                    ext=meta_ext,\n",
    "                                )\n",
    "                                done_meta.add(file)\n",
    "                            meta_df = read_meta(meta_file)\n",
    "                            meta_index.append(meta_df.index.tolist())\n",
    "                        export_level(\n",
    "                            file,\n",
    "                            block_obj,\n",
    "                            levels_path,\n",
    "                            ext=levels_ext,\n",
    "                            index=meta_index,\n",
    "                            dtype=dtype\n",
    "                        )\n",
    "                        done_levels.add(file)\n",
    "                        progress.refresh()\n",
    "        console.print(\"kbô :satisfied:\")\n",
    "    finally:\n",
    "        log_meta.write_text(\"\\n\".join(sorted(list(done_meta))))\n",
    "        log_levels.write_text(\"\\n\".join(sorted(list(done_levels))))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rfpy]",
   "language": "python",
   "name": "conda-env-rfpy-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
