{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser\n",
    "> Este módulo processa o arquivo bin e extrai os metadados e dados do espectro dos blocos, além de criar estatísticas das medições.\n",
    "  en: This module process the bin file extracting its metadata and spectrum levels besides extracting useful statistics.\n",
    "  fr: Ce module traite le fichier bin et extrait les métadonnées et les données spectrales des blocs, en plus de créer des statistiques de mesure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "# Insert in Path Project Directory\n",
    "sys.path.insert(0, str(Path().cwd().parent))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#exporti\n",
    "import os\n",
    "import gc\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "from collections import defaultdict\n",
    "from dataclasses import asdict, make_dataclass\n",
    "from fastcore.utils import parallel\n",
    "from fastcore.foundation import L, GetAttr\n",
    "from rfpye.constants import *\n",
    "from rfpye.blocks import MAIN_BLOCKS, BaseBlock\n",
    "from rfpye.utils import get_files, getattrs, bin2int, bin2str, cached\n",
    "from rfpye.cyparser import cy_extract_compressed\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rich import print\n",
    "# For scripts\n",
    "config = {\n",
    "    \"handlers\": [\n",
    "        {\n",
    "            \"sink\": \"parser.log\",\n",
    "            \"serialize\": True,\n",
    "            \"rotation\": \"1 month\",\n",
    "            \"compression\": \"zip\",\n",
    "            \"backtrace\": True,\n",
    "            \"diagnose\": True,\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "logger.configure(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = r\"binfiles\\v4\\rfeye002159_SLMA_bimestral_PEAK_210313_120302.bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processamento do Arquivo `.bin` e criação dos diferentes tipos de blocos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def evaluate_checksum(file, next_block, data_size) -> int:\n",
    "    \"\"\"Receives a byte_block and verify if the calculated checksum is equal to the one registed in the specific byte\"\"\"\n",
    "    checksum = np.frombuffer(file.read(4), np.uint32).item()\n",
    "    block_size = file.tell() - next_block\n",
    "    file.seek(-block_size, 1) # Go back to the beginning of the block\n",
    "    calculated_checksum = (\n",
    "            np.frombuffer(file.read(12+data_size), dtype=np.uint8)\n",
    "            .sum()\n",
    "            .astype(np.uint32)\n",
    "            .item()\n",
    "        )\n",
    "    file.seek(4,1) # skip checksum\n",
    "    return checksum if calculated_checksum == checksum else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def buffer2base_block(file, next_block: int) -> Union[BaseBlock, None]:\n",
    "    \"\"\"Receives an opened file buffer from the bin file and returns a dataclass with the attributes\n",
    "    'thread_id', 'size', 'type', 'data', 'checksum' or None in case any error is identified.\n",
    "    \"\"\"\n",
    "    thread_id = np.frombuffer(file.read(4), np.uint32).item()\n",
    "    data_size = np.frombuffer(file.read(4), np.uint32).item()\n",
    "    data_type = np.frombuffer(file.read(4), np.int32).item()\n",
    "    data_block = file.read(data_size)\n",
    "    if (checksum := evaluate_checksum(file, next_block, data_size)) is None:\n",
    "        return None           \n",
    "    return BaseBlock(thread_id, data_size, data_type, data_block, checksum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função a seguir recebe os bytes lidos do arquivo `.bin` e mapeia esses bytes em diferentes classes de acordo com o tipo de bloco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_block(file, next_block) -> Union[GetAttr, None]:\n",
    "    \"\"\"Receives a byte_block, and converts it into one of the main classes\n",
    "    Args: byte_block: A byte block directly returned from the file\n",
    "    Returns: The Instance of the Block Type or None in case of error\n",
    "    \"\"\"\n",
    "    if (base_block := buffer2base_block(file, next_block)) is None:\n",
    "        return None, None\n",
    "    block_type = base_block.type\n",
    "    constructor = MAIN_BLOCKS.get(block_type)\n",
    "    if not constructor:\n",
    "        _ = logger.log(\n",
    "            \"INFO\", f\"This block type constructor is not implemented: {block_type}\"\n",
    "        )\n",
    "        return None, None\n",
    "    block = constructor(base_block)\n",
    "    if getattr(block, \"gerror\", -1) != -1 or getattr(block, \"gps_status\", -1) == 0:\n",
    "        _ = logger.log(\"INFO\", f\"Block with error: {block_type}\")\n",
    "        return None, None  # spectral or gps blocks with error\n",
    "    return getattrs(block, KEY_ATTRS.get(block.type), as_tuple=True), block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função a seguir recebe os bytes lidos do arquivo `.bin` e mapeia esses bytes em diferentes classes de acordo com o tipo de bloco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def parse_bin(bin_file: Union[str, Path], precision=np.float32) -> dict:\n",
    "    \"\"\"Receives a CRFS binfile and returns a dictionary with the file metadata, a GPS Class and a list with the different Spectrum Classes\n",
    "    A block is a piece of the .bin file with a known start and end and that contains different types of information.\n",
    "    It has several fields: file_type, header, data and footer.\n",
    "    Each field has lengths and information defined in the documentation.\n",
    "    Args:\n",
    "        bin_file (Union[str, Path]): path to the bin file\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with the file metadata, file_version, string info, gps and spectrum blocks.\n",
    "    \"\"\"\n",
    "    bin_file = Path(bin_file)\n",
    "    meta = {}\n",
    "    fluxos = {}\n",
    "    gps = CrfsGPS()\n",
    "    with open(bin_file, mode=\"rb\") as file:\n",
    "        # The first block of the file is the header and is 36 bytes long.\n",
    "        header = file.read(BYTES_HEADER)\n",
    "        meta[\"filename\"] = bin_file.name\n",
    "        meta[\"file_version\"] = bin2int(header[:4])\n",
    "        meta[\"string\"] = bin2str(header[4:])\n",
    "        file_size = file.seek(0, 2)\n",
    "        file.seek(36, 0)\n",
    "        while (next_block := file.tell()) < file_size:\n",
    "            attrs, block = create_block(file, next_block)\n",
    "            if file.read(4) != b'UUUU':\n",
    "                logger.warning(\"End of block not found, skipping it\")\n",
    "                continue\n",
    "            if block is None: \n",
    "                continue\n",
    "            dtype = block.type\n",
    "            if dtype == 40:\n",
    "                for k in BLOCK_ATTRS.get(40, []):\n",
    "                    getattr(gps, f\"_{k}\").append(getattr(block, k))\n",
    "                continue\n",
    "            elif dtype in VECTOR_BLOCKS:\n",
    "                append_spec_data(attrs, fluxos, block, precision)\n",
    "            else:\n",
    "                meta.update(dict(zip(*attrs)))\n",
    "    meta[\"gps\"] = gps\n",
    "    meta[\"spectrum\"] = L(fluxos.values())\n",
    "    return meta                 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "@dataclass\n",
    "class CrfsGPS:\n",
    "    \"\"\"Class with the GPS Attributes from the CRFS Bin File\"\"\"\n",
    "\n",
    "    _gps_datetime: L = L()\n",
    "    _latitude: L = L()\n",
    "    _longitude: L = L()\n",
    "    _altitude: L = L()\n",
    "    _num_satellites: L = L()\n",
    "\n",
    "    @property\n",
    "    def latitude(self) -> float:\n",
    "        return np.median(self._latitude) if self._latitude else -1\n",
    "\n",
    "    @property\n",
    "    def longitude(self) -> float:\n",
    "        return np.median(self._longitude) if self._longitude else -1\n",
    "\n",
    "    @property\n",
    "    def altitude(self) -> float:\n",
    "        return np.median(self._altitude) if self._altitude else -1\n",
    "\n",
    "    @property\n",
    "    def num_satellites(self) -> float:\n",
    "        return np.median(self._num_satellites) if self._num_satellites else 0\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"GPS Data - Median of Coordinates: {self.latitude:.5f}:{self.longitude:.5f} Altitude: {self.altitude:.2f} #Satellites: {self.num_satellites:.1f}\"\n",
    "\n",
    "\n",
    "class CrfsSpectrum(GetAttr):\n",
    "    \"\"\"Class with the metadata and levels of a spectrum block from a CRFS Bin File\"\"\"\n",
    "\n",
    "    def __init__(self, metadata, precision=np.float32):\n",
    "        self.default = metadata\n",
    "        self._timestamp: L = L()\n",
    "        self._data: L = L()\n",
    "        self.precision = precision\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.levels.shape[0]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"SpectrumData - {repr(self.default)}\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"\"\"Blocks of Type: {self.type}, Thread_id: {self.thread_id}, Start: {self.start_mega} MHz, Stop: {self.stop_mega} MHz\"\"\"\n",
    "\n",
    "    @cached\n",
    "    def start_dateidx(self):\n",
    "        return self._timestamp[0].item()\n",
    "\n",
    "    @cached\n",
    "    def stop_dateidx(self):\n",
    "        return self._timestamp[-1].item()\n",
    "\n",
    "    @cached\n",
    "    def levels(self):\n",
    "        \"\"\"Return the spectrum levels\"\"\"\n",
    "        if self.type in UNCOMPRESSED:\n",
    "            levels = np.concatenate(self._data).reshape((-1, self.ndata))\n",
    "        elif self.type in COMPRESSED:\n",
    "            levels = cy_extract_compressed(\n",
    "                list(self._data),\n",
    "                len(self._data),\n",
    "                int(self.ndata),\n",
    "                int(self.thresh),\n",
    "                float(self.minimum),\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"The current block is not of type spectrum or it's not implemented yet\"\n",
    "            )\n",
    "        self._data = None\n",
    "        gc.collect()\n",
    "        if self.precision != np.float32:\n",
    "            levels = levels.astype(self.precision)\n",
    "        return levels\n",
    "\n",
    "    @cached\n",
    "    def frequencies(self) -> np.ndarray:\n",
    "        return np.linspace(self.start_mega, self.stop_mega, num=self.ndata)\n",
    "\n",
    "    def matrix(self):\n",
    "        \"\"\"Returns the matrix formed from the spectrum levels and timestamp\"\"\"\n",
    "        index = self._timestamp if len(self._timestamp) == len(self) else None\n",
    "        data = pd.DataFrame(self.levels, index=index, columns=self.frequencies)\n",
    "        data.columns.name = \"Frequencies\"\n",
    "        data.index.name = \"Time\"\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def check_block_exists(attrs, fluxos, precision):\n",
    "    \"\"\"Receives a dict of attributes and check if its values exist as keys in fluxos, otherwise create one and set to CrfsSpectrum Class\"\"\"\n",
    "    keys, vals = attrs\n",
    "    if vals not in fluxos:\n",
    "        metadata = make_dataclass('SpecData', fields=[(k,type(k)) for k in keys])\n",
    "        fluxos[vals] = CrfsSpectrum(metadata(*vals), precision)\n",
    "    return keys, vals, fluxos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def append_spec_data(attrs, fluxos, block, precision=np.float32) -> None:\n",
    "    keys, vals, fluxos = check_block_exists(attrs, fluxos, precision)\n",
    "    time = getattr(block, \"wallclock_datetime\", None)\n",
    "    data = getattr(block, \"levels\", None)\n",
    "    if time is not None:\n",
    "        fluxos[vals]._timestamp.append(time)\n",
    "    if data is not None:\n",
    "        fluxos[vals]._data.append(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rfpye]",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
