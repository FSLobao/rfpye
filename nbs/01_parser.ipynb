{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser\n",
    "> This module handles the processing and parsing of the metadata and spectrum data from blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "# Insert in Path Project Directory\n",
    "sys.path.insert(0, str(Path().cwd().parent))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# from multiprocessing import set_start_method, Pool\n",
    "# try:\n",
    "#     set_start_method(\"spawn\")\n",
    "# except RuntimeError:\n",
    "#     pass\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from fastcore.basics import uniqueify, partialler, listify\n",
    "from fastcore.utils import parallel\n",
    "from fastcore.foundation import L\n",
    "from rich.progress import track\n",
    "from rfpye.blocks import *\n",
    "from rfpye.utils import *\n",
    "from rfpye.constants import *\n",
    "from rfpye.cyparser import cy_extract_compressed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "path_type = Union[str, Any]\n",
    "bin_val = Union[int, bytes]\n",
    "bytes_encoded = Union[int, bytes]\n",
    "datetime_object = datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def parse_bin(\n",
    "    bin_file, \n",
    "    bytes_header: int = BYTES_HEADER, \n",
    "    marker: bytes = ENDMARKER, \n",
    "    slice_=None,\n",
    "    btypes: Iterable = MAIN_BLOCKS.keys()\n",
    "):\n",
    "    with open(bin_file, mode=\"rb\") as bfile:\n",
    "        # O primeiro bloco do arquivo é o cabeçalho e tem 36 bytes de tamanho.\n",
    "        header = bfile.read(bytes_header)\n",
    "        body = bfile.read()\n",
    "    if slice_ is not None:\n",
    "        assert (\n",
    "            slice_.start >= bytes_header\n",
    "        ), f\"The start of your slice has to be >= {bytes_header}, you passed {slice_.start} \"\n",
    "        body = body[slice_]\n",
    "    return {\n",
    "        \"file_version\": bin2int(header[:4]),\n",
    "        \"string\": bin2str(header[4:]),\n",
    "        \"blocks\": classify_blocks(body.split(marker), btypes=btypes),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processamento dos blocos\n",
    "A função seguinte `parse_bin` recebe um arquivo `.bin` e mapeia os blocos contidos nele retornando um dicionário que tem como chave o tipo de bloco e os valores como uma lista com os blocos extraídos sequencialmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# Python >=3.8 only\n",
    "# def binary_iter(bin_file: path_type, marker: bytes = b'\\x00UUUU', block_size: int = 4096) -> Iterator[bytes]:\n",
    "#     \"\"\"\n",
    "#     str, bytes, int > bytes\n",
    "#     :param bin_file: arquivo binario que contém os dados\n",
    "#     :param marker: separador de blocos\n",
    "#     :param block_size: tamanho em bytes que é \"lido\" por vez no arquivo, evitando problemas de memória\n",
    "#     :return: bloco em formato binario\n",
    "#     Gerador que fornece a partir de de um arquivo binário, um bloco binário por vez.\n",
    "\n",
    "#     \"\"\"\n",
    "#     with open(bin_file, mode='rb') as bfile:\n",
    "#         # O primeiro bloco do arquivo é o cabeçalho e tem 36 bytes de tamanho.\n",
    "#         yield bfile.read(36)\n",
    "#         # As demais partes podem prosseguir normalmente\n",
    "#         current = b''\n",
    "#         while block := bfile.read(block_size):\n",
    "#             current += block\n",
    "#             while (markerpos := current.find(marker)) > 0:\n",
    "#                 yield current[:markerpos]\n",
    "#                 current = current[markerpos + len(marker):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função a seguir mapeia o arquivo `.bin` nos devidos blocos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def classify_blocks(blocks: list, btypes: Iterable = MAIN_BLOCKS.keys()) -> Mapping[Tuple, Tuple]:\n",
    "    \"\"\"Receives an iterable L with binary blocks and returns a defaultdict with a tuple (block types, thread_id) as keys and a list of the Class Blocks as values\n",
    "    :param file: A string or pathlib.Path like path to a `.bin`file generated by CFRS - Logger\n",
    "    :return: A Dictionary with block types as keys and a list of the Class Blocks available as values\n",
    "    \"\"\"\n",
    "    map_block: Mapping[Tuple, L] = defaultdict(L)\n",
    "    btypes = listify(btypes)\n",
    "#     index = BYTES_HEADER\n",
    "    for block in blocks:\n",
    "        bloco_base = create_base_block(block)\n",
    "        btype, btid = bloco_base.type, bloco_base.thread_id\n",
    "#         stop = index + DATA_BLOCK_HEADER + len(bloco_base.data) + CHECKSUM\n",
    "        if btype not in btypes:\n",
    "#             index = stop + LEN_MARKER\n",
    "            continue\n",
    "        bloco = block_constructor(btype, bloco_base)\n",
    "        if btype in SPECTRAL_BLOCKS:\n",
    "            gerror = getattr(bloco, \"gerror\", -1)\n",
    "            if gerror != -1 or bloco._level_len != bloco.ndata:\n",
    "#                 index = stop + LEN_MARKER\n",
    "                continue\n",
    "        elif btype == GPS_BLOCK:\n",
    "            if not getattr(bloco, 'gps_status'): continue # equals to zero\n",
    "\n",
    "#         map_block[(btype, btid)].append(\n",
    "#             ((index, stop), bloco)\n",
    "#         )\n",
    "        map_block[(btype, btid)].append(bloco)\n",
    "#         index = stop + LEN_MARKER\n",
    "    return map_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def _extract_uncompressed(\n",
    "    blocks: Iterable, rows: int, cols: int, min_level: float, dtype=np.float16\n",
    "):\n",
    "    levels = np.full((rows, cols), min_level, dtype=dtype)\n",
    "    for b, block in enumerate(blocks):\n",
    "        levels[b] = block.block_data\n",
    "    return levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def extract_level(spectrum_blocks: L, dtype=np.float32) -> pd.DataFrame:\n",
    "    \"\"\"Receives a mapping `spectrum_blocks` and returns the Matrix with the Levels as values, Frequencies as columns and Block Number as index.\n",
    "    :param pivoted: If False, optionally returns an unpivoted version of the Matrix\n",
    "    \"\"\"\n",
    "    assert len(spectrum_blocks), f\"The spectrum block list is empty\"\n",
    "    spectrum_blocks = spectrum_blocks.itemgot(1)\n",
    "    block = spectrum_blocks[0]\n",
    "    assert block.type in (63, 64, 67, 68), f\"The input blocks are not spectral blocks\"\n",
    "    rows = len(spectrum_blocks)\n",
    "    min_level = block.offset - 127.5\n",
    "    if block.type in (63, 67):\n",
    "        cols = block.ndata\n",
    "        frequencies = getattr(block, \"frequencies\")\n",
    "        return pd.DataFrame(\n",
    "            _extract_uncompressed(spectrum_blocks, rows, cols, min_level, dtype),\n",
    "            columns=frequencies,\n",
    "        )\n",
    "    else:\n",
    "        cols = block.norig\n",
    "        thresh = block.thresh - 1\n",
    "        block_data = [b.block_data for b in spectrum_blocks]\n",
    "        frequencies = np.linspace(block.start_mega, block.stop_mega, num=cols)\n",
    "        levels = cy_extract_compressed(block_data, rows, cols, thresh, min_level)\n",
    "        if dtype != np.float32:\n",
    "            levels = levels.astype(dtype)\n",
    "        return pd.DataFrame(levels, columns=frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def meta2df(meta_list: Iterable, optimize: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"Receives and Iterable `metalist` with metadata and converts it to a DataFrame\"\"\"\n",
    "    df = pd.DataFrame(meta_list)\n",
    "    dt_features = [\"wallclock_datetime\"] if \"wallclock_datetime\" in df.columns else []\n",
    "    if optimize:\n",
    "        df = df_optimize(df, dt_features)\n",
    "    if dt_features:\n",
    "        df = df.set_index(\"wallclock_datetime\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def rowattrs(row, attrs):\n",
    "    return  {\n",
    "                **{\"start_byte\": row[0][0], \"stop_byte\": row[0][1]},\n",
    "                **getattrs(row[1], attrs),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def extract_metadata(blocos: L, attrs: list = None) -> pd.DataFrame:\n",
    "    \"\"\"Receives a list of blocks, extracts the metadata from them and return a DataFrame\"\"\"\n",
    "    func = partialler(getattrs, attrs=attrs)\n",
    "    df = meta2df(blocos.map(func))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def _export_meta(\n",
    "    parsed_blocks: tuple,\n",
    "    filter_attrs: Union[None, dict] = None,\n",
    "    save: bool = False,\n",
    "    stem: Union[str, Path] = None,\n",
    "    saida: Union[str, Path] = None,\n",
    "    ext: str = \".csv\",\n",
    ") -> Union[None, pd.DataFrame]:\n",
    "\n",
    "    (tipo, tid), blocos = parsed_blocks\n",
    "    if filter_attrs is None:\n",
    "        attrs = None\n",
    "    elif isinstance(filter_attrs, dict):\n",
    "        attrs = filter_attrs.get(tipo)\n",
    "    else:\n",
    "        raise ValueError(f\"Formato desconhecido do argumento {dict_attrs}:{type(dict_attrs)}, é esperado um dicionário\")\n",
    "    meta = extract_metadata(blocos, attrs)\n",
    "    if save:\n",
    "        dt_features = isinstance(meta.index, pd.DatetimeIndex)\n",
    "        name = f\"{stem}-B_{tipo}_TId_{tid}\"\n",
    "        if ext == \".csv\":\n",
    "            meta.to_csv(Path(saida) / f\"{name}{ext}\", index=bool(dt_features))\n",
    "        elif ext == \".xlsx\":\n",
    "            meta.to_excel(Path(saida) / f\"{name}{ext}\", index=bool(dt_features))\n",
    "        elif ext == \".fth\":\n",
    "            meta.to_feather(Path(saida) / f\"{name}{ext}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Extension {ext} not implemented\")\n",
    "    return tipo, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def export_metadata(\n",
    "    parsed_bin: dict, \n",
    "    filter_attrs: Union[None, dict] = None,\n",
    "    save: bool = False, \n",
    "    stem: Union[str, Path] = None, \n",
    "    saida: Union[str, Path] = None, \n",
    "    ext: str = \".csv\"\n",
    ") -> None:\n",
    "\n",
    "    file_version, string, blocks = parsed_bin.values()\n",
    "    func = partialler(_export_meta, filter_attrs=filter_attrs, save=save, stem=stem, saida=saida, ext=ext)\n",
    "    func.__module__ = _export_meta.__module__\n",
    "    metas = parallel(func, list(blocks.items()), threadpool=True, n_workers=os.cpu_count())\n",
    "    output = defaultdict(list)\n",
    "    for t,m in metas: output[t].append(m)    \n",
    "    return {k:pd.concat(v) for k,v in output.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def _export_level(\n",
    "    parsed_blocks: tuple,\n",
    "    stem: Union[str, Path],\n",
    "    saida: Union[str, Path],\n",
    "    ext: str = \".fth\",\n",
    "    dtype: Union[str, np.dtype] = np.float16,\n",
    ") -> Union[None, pd.DataFrame]:\n",
    "\n",
    "    ((tipo, tid), blocos), index = parsed_blocks\n",
    "    assert (\n",
    "        tipo in SPECTRAL_BLOCKS\n",
    "    ), \"Tentativa de extrair espectro de um bloco que não é espectral\"\n",
    "\n",
    "    saida = Path(saida)\n",
    "    level = extract_level(blocos, dtype)\n",
    "    if index is not None:\n",
    "        level.index = index\n",
    "    dt_features = isinstance(level.index, pd.DatetimeIndex)\n",
    "\n",
    "    name = f\"{stem}-B_{tipo}_TId_{tid}\"\n",
    "    if ext == \".fth\":\n",
    "        if index is not None:\n",
    "            level = level.reset_index()\n",
    "        level.columns = [str(c) for c in level.columns]\n",
    "        level.to_feather(f\"{saida}/{name}{ext}\")\n",
    "    else:\n",
    "        raise ValueError(f\"Extension {ext} not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports \n",
    "def export_level(\n",
    "    stem: Union[str, Path],\n",
    "    parsed_bin: dict,\n",
    "    saida: Union[str, Path],\n",
    "    ext: str = \".fth\",\n",
    "    index: pd.DatetimeIndex = None,\n",
    "    dtype: Union[str, np.dtype] = np.float16,\n",
    ") -> None:\n",
    "\n",
    "    _, _, blocks = parsed_bin.values()\n",
    "    blocks = [((t, i), b) for (t, i), b in blocks.items() if t in SPECTRAL_BLOCKS]\n",
    "    if not index:\n",
    "        index = [None] * len(blocks)\n",
    "    items = list(zip(blocks, index))\n",
    "    func = partialler(_export_level, stem=stem, saida=saida, ext=ext, dtype=dtype)\n",
    "    func.__module__ = _export_meta.__module__\n",
    "    parallel(func, items, n_workers=os.cpu_count(), pause=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_filter.ipynb.\n",
      "Converted 01_parser.ipynb.\n",
      "Converted 02_utils.ipynb.\n",
      "Converted 03_blocks.ipynb.\n",
      "Converted 04_constants.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rfpy]",
   "language": "python",
   "name": "conda-env-rfpy-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
