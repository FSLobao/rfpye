{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "# Insert in Path Project Directory\n",
    "sys.path.insert(0, str(Path().cwd().parent))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from fastcore.basics import uniqueify\n",
    "from fastcore.utils import parallel\n",
    "from fastcore.foundation import L\n",
    "from rfpy.blocks import *\n",
    "from rfpy.utils import bin2int, bin2str, optimize, getattrs\n",
    "from rfpy.constants import *\n",
    "from rfpy.cyparser import cy_extract_compressed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "from datetime import datetime\n",
    "\n",
    "path_type = Union[str, Any]\n",
    "bin_val = Union[int, bytes]\n",
    "bytes_encoded = Union[int, bytes]\n",
    "datetime_object = datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonte: # https://github.com/fastai/fastai/blob/master/fastai/data/transforms.py#L26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_files(p, fs, extensions=None):\n",
    "    p = Path(p)\n",
    "    res = [p/f for f in fs if not f.startswith('.')\n",
    "           and ((not extensions) or f'.{f.split(\".\")[-1].lower()}' in extensions)]\n",
    "    return res\n",
    "\n",
    "def get_files(path, extensions=None, recurse=True, folders=None, followlinks=True):\n",
    "    \"Get all the files in `path` with optional `extensions`, optionally with `recurse`, only in `folders`, if specified.\"\n",
    "    path = Path(path)\n",
    "    folders=L(folders)\n",
    "    if extensions is not None:\n",
    "        extensions = set(uniqueify(extensions))\n",
    "        extensions = {e.lower() for e in extensions}\n",
    "    if recurse:\n",
    "        res = []\n",
    "        for i,(p,d,f) in enumerate(os.walk(path, followlinks=followlinks)): # returns (dirpath, dirnames, filenames)\n",
    "            if len(folders) !=0 and i==0: d[:] = [o for o in d if o in folders]\n",
    "            else:                         d[:] = [o for o in d if not o.startswith('.')]\n",
    "            if len(folders) !=0 and i==0 and '.' not in folders: continue\n",
    "            res += _get_files(p, f, extensions)\n",
    "    else:\n",
    "        f = [o.name for o in os.scandir(path) if o.is_file()]\n",
    "        res = _get_files(path, f, extensions)\n",
    "    return L(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def parse_bin(bin_file, bytes_header: int = BYTES_HEADER, marker: bytes = ENDMARKER, slice_=None):\n",
    "    with open(bin_file, mode='rb') as bfile:\n",
    "        # O primeiro bloco do arquivo é o cabeçalho e tem 36 bytes de tamanho.\n",
    "        header = bfile.read(bytes_header)\n",
    "        body = bfile.read()\n",
    "    if slice_ is not None:\n",
    "        assert slice_.start >= bytes_header, f\"The start of your slice has to be >= {bytes_header}, you passed {slice_.start} \"\n",
    "        body = body[slice_]\n",
    "    return {'file_version': bin2int(header[:4]), 'string': bin2str(header[4:]), 'blocks': classify_blocks(L(body.split(marker)))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Python >=3.8 only\n",
    "def binary_iter(bin_file: path_type, marker: bytes = b'\\x00UUUU', block_size: int = 4096) -> Iterator[bytes]:\n",
    "    \"\"\"\n",
    "    str, bytes, int > bytes\n",
    "    :param bin_file: arquivo binario que contém os dados\n",
    "    :param marker: separador de blocos\n",
    "    :param block_size: tamanho em bytes que é \"lido\" por vez no arquivo, evitando problemas de memória\n",
    "    :return: bloco em formato binario\n",
    "    Gerador que fornece a partir de de um arquivo binário, um bloco binário por vez.\n",
    "\n",
    "    \"\"\"\n",
    "    with open(bin_file, mode='rb') as bfile:\n",
    "        # O primeiro bloco do arquivo é o cabeçalho e tem 36 bytes de tamanho.\n",
    "        yield bfile.read(36)\n",
    "        # As demais partes podem prosseguir normalmente\n",
    "        current = b''\n",
    "        while block := bfile.read(block_size):\n",
    "            current += block\n",
    "            while (markerpos := current.find(marker)) > 0:\n",
    "                yield current[:markerpos]\n",
    "                current = current[markerpos + len(marker):]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função a seguir mapeia o arquivo `.bin` nos devidos blocos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def classify_blocks(blocks: L)->Mapping[int,L]:\n",
    "    \"\"\"Receives an iterable L with binary blocks and returns a defaultdict with a tuple (block types, thread_id) as keys and a list of the Class Blocks as values\n",
    "        :param file: A string or pathlib.Path like path to a `.bin`file generated by CFRS - Logger\n",
    "        :return: A Dictionary with block types as keys and a list of the Class Blocks available as values\n",
    "    \"\"\"\n",
    "    map_block = defaultdict(L)\n",
    "    index = BYTES_HEADER\n",
    "    for bloco in blocks:\n",
    "        bloco = create_base_block(bloco)\n",
    "        btype, btid = bloco.type, bloco.thread_id\n",
    "        if btype == btid == 0: #Ignore empty block at the beginning if present\n",
    "            continue\n",
    "        stop = index + DATA_BLOCK_HEADER + len(bloco.data) + CHECKSUM\n",
    "        map_block[(btype, btid)].append(((index,stop), block_constructor(btype, bloco)))\n",
    "        #map_block[(btype, btid)].append(block_constructor(btype, bloco))\n",
    "        #map_block[btype].append(((index,stop), block_constructor(btype, bloco)))\n",
    "        index = stop + LEN_MARKER\n",
    "    return map_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def decode_compressed_block(block):\n",
    "    MIN = block.offset - 127.5\n",
    "    RUN = 255\n",
    "    ESC = 254\n",
    "    src = block.data[block.start:block.stop]\n",
    "    nsrc = len(src)\n",
    "    thresh = block.thresh\n",
    "    dest = np.full(block.norig, MIN, dtype=np.float16)\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < nsrc:\n",
    "        ib = src[i] \n",
    "        i+=1\n",
    "        if ib == RUN:\n",
    "            nrun = src[i] \n",
    "            i+=1\n",
    "            dest[j:j+nrun] = MIN + thresh/2.\n",
    "            j+=nrun\n",
    "        elif ib == ESC:\n",
    "            # next value is literal\n",
    "            dest[j] = MIN + src[i]/2.\n",
    "            i+=1 ; j+=1\n",
    "        else:\n",
    "            # value\n",
    "            dest[j] = MIN + ib/2.\n",
    "            j+=1\n",
    "    return dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def run_length_decode5(dest, src, nsrc, thresh, MIN):\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < nsrc:\n",
    "        ib = src[i] \n",
    "        i+=1\n",
    "        if ib == RUN:\n",
    "            nrun = src[i] \n",
    "            i+=1\n",
    "            dest[j:j+nrun] = thresh #dest is now a numpy array\n",
    "            j+=nrun\n",
    "        elif ib == ESC:\n",
    "            # next value is literal\n",
    "            dest[j] = src[i]/2. + MIN\n",
    "            i+=1 ; j+=1\n",
    "        else:\n",
    "            # value\n",
    "            dest[j] = ib/2. + MIN\n",
    "            j+=1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _extract_uncompressed(blocks: Iterable, rows: int, cols: int, MIN: float):\n",
    "    levels = np.full((rows, cols), MIN, dtype=np.float16)\n",
    "    for b, block in enumerate(blocks):\n",
    "        levels[b] = block.block_data\n",
    "    return levels    \n",
    "\n",
    "\n",
    "def extract_levels(spectrum_blocks: Iterable)->pd.DataFrame:\n",
    "    \"\"\"Receives a mapping `spectrum_blocks` and returns the Matrix with the Levels as values, Frequencies as columns and Block Number as index.\n",
    "       :param pivoted: If False, optionally returns an unpivoted version of the Matrix\n",
    "    \"\"\"\n",
    "    assert len(spectrum_blocks), f\"The spectrum block list is empty\"\n",
    "    spectrum_blocks = spectrum_blocks.itemgot(1)\n",
    "    block = spectrum_blocks[0]\n",
    "    assert block.type in (63, 64, 67, 68), f\"The input blocks are not spectral blocks\" \n",
    "    rows = len(spectrum_blocks)\n",
    "    MIN = block.offset - 127.5\n",
    "    if block.type in (63,67):\n",
    "        cols = block.ndata\n",
    "        frequencies = getattr(spectrum_blocks[0], 'frequencies')\n",
    "        return pd.DataFrame(_extract_uncompressed(spectrum_blocks, rows, cols, MIN), columns=frequencies)\n",
    "    else:\n",
    "        cols = block.norig\n",
    "        thresh = block.thresh - 1\n",
    "        block_data = [b.block_data for b in spectrum_blocks] \n",
    "        frequencies = np.linspace(block.start_mega, block.stop_mega, num=cols)\n",
    "        return pd.DataFrame(cy_extract_compressed(block_data, rows, cols, thresh, MIN), columns=frequencies)\n",
    "#     if not pivoted:\n",
    "#         unpivot = np.array([np.repeat(np.arange(levels.shape[1]), levels.shape[0]),\n",
    "#                             np.tile(frequencies, levels.shape[0]),\n",
    "#                             levels.flatten()]).T\n",
    "#         if not dtypes:\n",
    "#             dtypes = LEVELS\n",
    "#         return pd.DataFrame(unpivot, columns=LEVELS.keys()).astype(dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def meta2df(meta_list: Iterable, optimize: bool = False)->pd.DataFrame:\n",
    "    \"\"\"Receives and Iterable `metalist` with metadata and converts it to a DataFrame\"\"\"\n",
    "    df = pd.DataFrame(meta_list)\n",
    "    dt_features = ['wallclock_datetime'] if 'wallclock_datetime' in df.columns else []\n",
    "    if optimize:\n",
    "        df = optimize(df, dt_features)\n",
    "    if dt_features:\n",
    "        df = df.set_index('wallclock_datetime')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def extract_metadata(map_blocks: Mapping[int,L], subset=MAIN_BLOCKS)->pd.DataFrame:\n",
    "    \"\"\"Receives a Mapping with the different `. bin` Blocks and extracts the metadata from them excluding spectral data.\n",
    "    \"\"\"\n",
    "    metadata = {}\n",
    "    file_version, string, blocks = map_blocks.values()\n",
    "    for (tipo, tid), blocos in blocks.items():\n",
    "        if tipo not in subset:\n",
    "            continue\n",
    "        df = meta2df(blocos.map(lambda item: {**{'start_byte': item[0][0], 'stop_byte': item[0][1]}, **getattrs(item[1])}))        \n",
    "        df['file_version'] = file_version\n",
    "        metadata[(tipo, tid)] = df\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_main.ipynb.\n",
      "Converted 01_parser.ipynb.\n",
      "Converted 02_utils.ipynb.\n",
      "Converted 03_blocks.ipynb.\n",
      "Converted 04_constants.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rfpy]",
   "language": "python",
   "name": "conda-env-rfpy-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
